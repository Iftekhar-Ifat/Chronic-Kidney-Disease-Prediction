{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_dataset = pd.read_csv('./kidney_disease.csv')\n",
    "\n",
    "# making csv a pandas dataframe\n",
    "dataframe = pd.DataFrame(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seeds\n",
    "np.random.seed(42)\n",
    "\n",
    "# dropping 'id' column\n",
    "dataframe.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# rename column names to make it more user-friendly\n",
    "dataframe.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',\n",
    "              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',\n",
    "              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',\n",
    "              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',\n",
    "              'aanemia', 'class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting necessary columns to numerical type\n",
    "dataframe['packed_cell_volume'] = pd.to_numeric(dataframe['packed_cell_volume'], errors='coerce')\n",
    "dataframe['white_blood_cell_count'] = pd.to_numeric(dataframe['white_blood_cell_count'], errors='coerce')\n",
    "dataframe['red_blood_cell_count'] = pd.to_numeric(dataframe['red_blood_cell_count'], errors='coerce')\n",
    "\n",
    "\n",
    "# Extracting categorical and numerical columns\n",
    "cat_cols = [col for col in dataframe.columns if dataframe[col].dtype == 'object']\n",
    "num_cols = [col for col in dataframe.columns if dataframe[col].dtype != 'object']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking numerical features distribution\n",
    "plt.figure(figsize = (20, 15))\n",
    "plotnumber = 1\n",
    "for column in num_cols:\n",
    "    if plotnumber <= 14:\n",
    "        ax = plt.subplot(3, 5, plotnumber)\n",
    "        sns.histplot (dataframe[column],color='blue',kde=True, stat=\"density\", linewidth=0)\n",
    "        plt.xlabel(column)\n",
    "    \n",
    "        \n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at categorical columns\n",
    "\n",
    "plt.figure(figsize = (20, 15))\n",
    "plotnumber = 1\n",
    "for column in cat_cols:\n",
    "    if plotnumber <= 11:\n",
    "        ax = plt.subplot(3, 4, plotnumber)\n",
    "        sns.countplot(x = dataframe[column], palette = 'deep',color='black')\n",
    "        plt.xlabel(column)\n",
    "        \n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace incorrect values\n",
    "dataframe['diabetes_mellitus'].replace(to_replace = {'\\tno':'no','\\tyes':'yes',' yes':'yes'},inplace=True)\n",
    "dataframe['coronary_artery_disease'] = dataframe['coronary_artery_disease'].replace(to_replace = '\\tno', value='no')\n",
    "dataframe['class'] = dataframe['class'].replace(to_replace = {'ckd\\t': 'ckd', 'notckd': 'not ckd'})\n",
    "\n",
    "# replacing 'ckd' with 0 and 'not ckd' with 1\n",
    "dataframe['class'] = dataframe['class'].map({'ckd': 0, 'not ckd': 1})\n",
    "\n",
    "# making 'class' column into a numerical column\n",
    "dataframe['class'] = pd.to_numeric(dataframe['class'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling null values, we will use two methods, random sampling for higher null values and \n",
    "# mean/mode sampling for lower null values\n",
    "\n",
    "def random_value_imputation(feature):\n",
    "    random_sample = dataframe[feature].dropna().sample(dataframe[feature].isna().sum())\n",
    "    random_sample.index = dataframe[dataframe[feature].isnull()].index\n",
    "    dataframe.loc[dataframe[feature].isnull(), feature] = random_sample\n",
    "    \n",
    "def impute_mode(feature):\n",
    "    mode = dataframe[feature].mode()[0]\n",
    "    dataframe[feature] = dataframe[feature].fillna(mode)\n",
    "\n",
    "\n",
    "\n",
    "# filling num_cols null values using random sampling method\n",
    "for col in num_cols:\n",
    "    random_value_imputation(col)\n",
    "\n",
    "\n",
    "# filling \"red_blood_cells\" and \"pus_cell\" using random sampling method and rest of cat_cols using mode imputation\n",
    "random_value_imputation('red_blood_cells')\n",
    "random_value_imputation('pus_cell')\n",
    "\n",
    "for col in cat_cols:\n",
    "    impute_mode(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "for col in cat_cols:\n",
    "    dataframe[col] = le.fit_transform(dataframe[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature matrix\n",
    "X = dataframe.drop(\"class\", axis=1)     # everything except 'class' column\n",
    "\n",
    "# Y = lables\n",
    "Y = dataframe['class']      # only 'class' column\n",
    "\n",
    "# spliting data to train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randorm Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rand_forest = RandomForestClassifier()\n",
    "\n",
    "clf_rand_forest.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "clf_svm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression(solver='lbfgs', max_iter=2500);\n",
    "clf_log.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG_Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf_xgb = XGBClassifier(objective = 'binary:logistic', \n",
    "                        learning_rate = 0.5, \n",
    "                        max_depth = 5, \n",
    "                        n_estimators = 150, \n",
    "                        eval_metric='mlogloss',  \n",
    "                        use_label_encoder=False)\n",
    "\n",
    "clf_xgb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dtc = DecisionTreeClassifier()\n",
    "clf_dtc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf_ada = AdaBoostClassifier(base_estimator = clf_dtc)\n",
    "clf_ada.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_rand_forest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rand_forest_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_rand_forest.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for Random Forest Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_svm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svm_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_svm.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for SVM Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_gnb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "gnb_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_gnb.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for Naive Bayes Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_log.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "log_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_log.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for Logistic Regression Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_knn.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "knn_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_knn.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for KNN Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG_Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_xgb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "xgb_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_xgb.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for XG_Boost Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_dtc.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "dtc_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_dtc.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for Decision Tree Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf_ada.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "ada_acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_ada.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from matplotlib import pyplot\n",
    "fig, (ax2) = plt.subplots(figsize = (8,6))\n",
    "        #roc-curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(Y_test,Y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "ax2.legend(loc = 'best')\n",
    "plt.title('ROC curve for Ada Boost Classifier ')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        #find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model' : [ 'Random Forest Classifier', 'SVM Classifier', 'Naive Bayes Classifier','KNN Classifier',\n",
    "             'XG_Boost Classifier', 'Decision Tree Classifier', 'Ada_Boost Classifier'],\n",
    "    'Score' : [rand_forest_acc, svm_acc, gnb_acc, knn_acc, xgb_acc, dtc_acc, ada_acc]\n",
    "})\n",
    "\n",
    "\n",
    "sorted_models = models.sort_values(by = 'Score', ascending = True)\n",
    "\n",
    "fig = px.bar(data_frame = sorted_models, x = 'Score', y = 'Model',\n",
    "       title = 'Models Comparison')\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
